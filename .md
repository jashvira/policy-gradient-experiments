[1mdiff --git a/experiments/grpo/config_schema.py b/experiments/grpo/config_schema.py[m
[1mindex 774b203..7ac5052 100644[m
[1m--- a/experiments/grpo/config_schema.py[m
[1m+++ b/experiments/grpo/config_schema.py[m
[36m@@ -48,6 +48,7 @@[m [mclass GRPOTrainConfig(BaseTrainConfig):[m
     learning_rate: float = 1e-5  # Lower learning rate for RL[m
     lr: float = 1e-5  # Alias for learning_rate[m
     adam_beta2: float = 0.95  # GRPO-specific beta2 value[m
[32m+[m[32m    adam_fused: Optional[bool] = None  # Enable fused AdamW when supported[m
     gradient_accumulation_steps: int = 128  # Microbatch size = 2 for H100[m
     gpu_memory_utilization: float = 0.85[m
     vllm_gpu_mem_utilization: float = 0.85  # Override base[m
[36m@@ -56,7 +57,8 @@[m [mclass GRPOTrainConfig(BaseTrainConfig):[m
     data_path: Optional[str] = None  # Defaults to MATH dataset if None[m
     val_every_grpo_steps: int = 10  # Evaluate every N GRPO steps[m
     val_rollout_batch_size: int = 64  # Smaller batch for validation[m
[31m-    val_samples: int = 1024  # Number of validation samples for robust evaluation[m
[32m+[m[32m    # Number of validation samples or "all" to use the entire validation set[m
[32m+[m[32m    val_samples: Union[int, str] = 1024[m
     save_every_grpo_steps: int = 50  # Save checkpoint every N steps[m
 [m
     # Model and checkpoint settings[m
